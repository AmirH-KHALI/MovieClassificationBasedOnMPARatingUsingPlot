# -*- coding: utf-8 -*-
"""MovieClassificationBasedOnMPAUsingPlot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qY0hiMM7aLmQvKw6I3whFgNK5Y8dySWI

# **Imports**
"""

!pip install -q -U "tensorflow-text==2.8.*"
!pip install -q tf-models-official==2.7.0
!pip install datasets
!pip install transformers datasets
!pip install autokeras

import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import string
import os.path
import matplotlib.pyplot as plt
import ast

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize
from collections import Counter

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization
from tensorflow.keras import layers
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import TextVectorization
import autokeras as ak
from tensorflow.keras.models import load_model

from google.colab import drive, auth
# Mount Google Drive
drive.mount("/content/drive")

base_dir = './drive/MyDrive/University/00012-NLP/MovieClassification/'
# base_dir = './'

"""# **Data Collection (Phase 1, Part 1)**"""

raw_list = []

def crawl(mpa):
    for current_page in tqdm(range(1, 9952, 50)):
        response = requests.get('https://www.imdb.com/search/title/?' 
                                + 'title_type=feature,tv_movie,tv_special,documentary,short,tv_short' 
                                + '&certificates=US%3A' + mpa 
                                + '&start=' + str(current_page))

        soup = BeautifulSoup(response.text, 'html.parser')

        for i in range(len(soup.select('h3.lister-item-header a'))):
            title = soup.select('h3.lister-item-header a')[i].get_text()
            plot = soup.select('p.text-muted')[2 * i + 1].get_text()
            raw_list.append([title, mpa, plot])

crawl('G')

crawl('PG')

crawl('PG-13')

crawl('R')

raw_data = pd.DataFrame(raw_list, columns = ['Title', 'MPA', 'Plot'])
raw_data.Plot = raw_data.Plot.apply(lambda p: p.replace('\n', ''))
raw_data.head()

if not os.path.isdir(base_dir + 'data/'):
    os.mkdir(base_dir + 'data/')

if not os.path.isdir(base_dir + 'data/raw/'):
    os.mkdir(base_dir + 'data/raw/')

raw_data.to_csv(base_dir + 'data/raw/data.csv')

"""# **Preprocessing (Phase 1, Part 2)**"""

df = pd.read_csv(base_dir + 'data/raw/data.csv', index_col=0)
df

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

df = df.drop(df[df.Plot == 'Add a Plot'].index)
df = df.reset_index().drop(columns = 'index')
df['Plot'] = df['Plot'].str.replace('See full summary', '')

df['Normalized_Plot'] = df['Plot'].str.lower()
df['Normalized_Plot'] = df['Normalized_Plot'].str.translate(str.maketrans('', '', string.punctuation + '»' + '«'))
df['Normalized_Plot'] = df['Normalized_Plot'].str.translate(str.maketrans('', '', string.digits))
df['Normalized_Plot'] = df['Normalized_Plot'].apply(word_tokenize)
df['Normalized_Plot'] = df['Normalized_Plot'].apply(lambda lst : [word for word in lst if word not in set(stopwords.words('english'))])
df['Normalized_Plot'] = df['Normalized_Plot'].apply(lambda lst : [WordNetLemmatizer().lemmatize(w) for w in lst])

if not os.path.isdir(base_dir + 'data/'):
    os.mkdir(base_dir + 'data/')

if not os.path.isdir(base_dir + 'data/cleaned/'):
    os.mkdir(base_dir + 'data/cleaned/')

df.to_csv(base_dir + 'data/cleaned/data.csv')

"""# **Statistics (Phase 1, Part 3)**"""

df = pd.read_csv(base_dir + 'data/cleaned/data.csv', index_col=0)
df

MPA_counts = {MPA : sum(df['MPA'] == MPA) for MPA in ['G', 'PG', 'PG-13', 'R']}
plt.bar(MPA_counts.keys(), MPA_counts.values())
plt.show()

counter = Counter()
df['Normalized_Plot'].apply(lambda lst : ast.literal_eval(lst)).apply(counter.update)

counter_list = [(k, v) for k, v in sorted(counter.items(), key=lambda item: item[1], reverse=True)]
counter_list = counter_list[:10]
x, y = zip(*counter_list)
plt.figure(figsize=(10, 5), facecolor=None)
plt.bar(x, y)
plt.show()

print('Sentence count:', df['Plot'].apply(sent_tokenize).apply(len).sum())
print('All words(preprocessed words):', sum(counter.values()))
print('Unique words:', len(counter))

"""# **Train Preprocessing (Phase 2)**"""

df = pd.read_csv(base_dir + 'data/cleaned/data.csv', index_col=0)
df['Normalized_Plot'] = df['Normalized_Plot'].apply(lambda lst : ast.literal_eval(lst))
df

mpa_dict = {'G': 0, 'PG': 1, 'PG-13': 2, 'R': 3}
df['label'] = [mpa_dict[x] for x in df['MPA']]
df['text'] = [' '.join(x) for x in df['Normalized_Plot']]

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(df['text'], df['label'])

y_train = to_categorical(y_train, num_classes=4)
y_test = to_categorical(y_test, num_classes=4)

x_train.shape, y_train.shape, x_test.shape, y_test.shape

"""# **Simple Text Classification Model (Phase 2, Part 2)**"""

max_features = 20000
embedding_dim = 128
sequence_length = 500

vectorize_layer = TextVectorization(
    max_tokens=max_features,
    output_mode="int",
    output_sequence_length=sequence_length,
)
vectorize_layer.adapt(x_train)

text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')
x = vectorize_layer(text_input)
x = layers.Embedding(max_features + 1, embedding_dim)(x)

x = layers.Dropout(0.5)(x)

# Conv1D + global max pooling
x = layers.Conv1D(128, 7, padding='valid', activation='relu', strides=3)(x)
x = layers.Conv1D(128, 7, padding='valid', activation='relu', strides=3)(x)
x = layers.GlobalMaxPooling1D()(x)

# We add a vanilla hidden layer:
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# We project onto a single unit output layer, and squash it with a sigmoid:
predictions = layers.Dense(4, activation='sigmoid', name='predictions')(x)

model = tf.keras.Model(text_input, predictions)

# Compile the model with binary crossentropy loss and an adam optimizer.
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(tf.constant(x_train), y_train, validation_split=0.2, epochs=3)

model.save(base_dir + 'models/simple_text_classification', save_format = "tf")
loaded_model = load_model(base_dir + 'models/simple_text_classification')
loaded_model.evaluate(tf.constant(x_test), y_test)

"""# **Improved Text Classification Models (Phase 2, Part 3)**

## **Text Classification Using Transformers layer**
"""

max_features = 20000
embedding_dim = 128
sequence_length = 500

num_heads = 2  # Number of attention heads
ff_dim = 32  # Hidden layer size in feed forward network inside transformer

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential(
            [layers.Dense(ff_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super(TokenAndPositionEmbedding, self).__init__()
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')
x = vectorize_layer(text_input)

embedding_layer = TokenAndPositionEmbedding(sequence_length, max_features, embedding_dim)
x = embedding_layer(x)

transformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)
x = transformer_block(x)

x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.1)(x)

x = layers.Dense(20, activation='relu')(x)
x = layers.Dropout(0.1)(x)

outputs = layers.Dense(4, activation='softmax')(x)

model = tf.keras.Model(inputs=text_input, outputs=outputs)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=32, epochs=3, validation_split=0.2)

model.save(base_dir + 'models/text_classification_using_transformers', save_format = "tf")
loaded_model = load_model(base_dir + 'models/text_classification_using_transformers')
loaded_model.evaluate(tf.constant(x_test), y_test)

"""## **Text Classification Using Bert**"""

bert_model_name = 'small_bert/bert_en_uncased_L-10_H-512_A-8'

tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1'
tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'

print(f'BERT model selected           : {tfhub_handle_encoder}')
print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')

bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)

text_test = ['Hi bitchez!']
text_preprocessed = bert_preprocess_model(text_test)

print(text_test)
print(f'Keys       : {list(text_preprocessed.keys())}')
print(f'Shape      : {text_preprocessed["input_word_ids"].shape}')
print(f'Word Ids   : {text_preprocessed["input_word_ids"][0, : 20]}')
print(f'Input Mask : {text_preprocessed["input_mask"][0, : 20]}')
print(f'Type Ids   : {text_preprocessed["input_type_ids"][0, : 20]}')

bert_model = hub.KerasLayer(tfhub_handle_encoder)

bert_results = bert_model(text_preprocessed)

print(f'Loaded BERT: {tfhub_handle_encoder}')
print(f'Pooled Outputs Shape:{bert_results["pooled_output"].shape}')
print(f'Pooled Outputs Values:{bert_results["pooled_output"][0, :12]}')
print(f'Sequence Outputs Shape:{bert_results["sequence_output"].shape}')
print(f'Sequence Outputs Values:{bert_results["sequence_output"][0, :12]}')

def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dense(4, activation='softmax', name='classifier')(net)
    return tf.keras.Model(text_input, net)

classifier_model = build_classifier_model()
bert_raw_result = classifier_model(tf.constant(text_test))
print(tf.sigmoid(bert_raw_result))
classifier_model.summary()

classifier_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])

classifier_model.fit(x_train, y_train, validation_split=.3, epochs=3, batch_size=64)

classifier_model.save(base_dir + 'models/text_classification_using_bert', save_format = "tf")
loaded_model = load_model(base_dir + 'models/text_classification_using_bert')
loaded_model.evaluate(tf.constant(x_test), y_test)

"""## **Find The Best Performing Model**"""

X_train = np.array(x_train)
X_test = np.array(x_test)

clf = ak.TextClassifier(max_trials=3)

clf.fit(X_train, y_train, validation_split=0.3, epochs=3)

model = clf.export_model()
model.summary()

model.save(base_dir + "models/clf_best_performing", save_format = "tf")
loaded_model = load_model(base_dir + "models/clf_best_performing", custom_objects = ak.CUSTOM_OBJECTS)
loaded_model.evaluate(X_test, y_test)